* mgcpp

A CUDA based C++ Multi GPU Linear Algebra Library

** Introduction
   mgcpp is a multi GPU linear algebra library.
   It is a wrapper for various CUDA libraries in standard C++.
 
** Status
   This library is heavily under development and in pre-alpha state.
   For contribution, refer to TODO or contact me.
   
** Example
   #+NAME: mult_example 
   #+BEGIN_SRC cpp
using namespace mgcpp;

int const device = 0;

gpu::matrix<float, device> A(2, 4, 2);
gpu::matrix<float, device> B(4, 3, 4);

gpu::matrix<float, device> C = A * B;

/* or */

auto C = eval(A * B);
   #+END_SRC
   
   The above code invokes the highly optimized cuBLAS library's gemm function.
   All operation are lazely computed using expression templates.
   GPU computation kernels are also called the least possible.

** Build
   #+NAME: installing
   #+BEGIN_SRC sh
git clone --recursive https://github.com/Red-Portal/mgcpp.git
cmake -G "<Generator>"
make -j4
make install
   #+END_SRC
   
   for building without MAGMA or cuSPARSE,

   #+NAME: installing
   #+BEGIN_SRC sh
git clone --recursive https://github.com/Red-Portal/mgcpp.git
cmake -DUSE_MAGMA=OFF -DUSE_CUSPARSE=OFF -G "<Generator>"
make -j4
make install
   #+END_SRC

** Used Libraries
   These dependencies are optional. 
   Not including them might result in some featrues not working properly.

   - cuSPARSE for sparse operations
   - [[https://github.com/kjbartel/magma][MAGMA]] for hight level linear algebra operations.
   
** Dependencies
   - cmake
   - gcc (>= 6) or clang (>= 3.9) or Visual Studio (>= 15)

   - [[https://github.com/ned14/outcome][outcome]]
   - cuda (>= 8.0)
   - cuBLAS
   - gtest (optional)
     
** TODO
   - [X] Finish thread context class for managing threads.
   - [ ] Support msvc build.
   - [ ] Add BLAS operations.
   - [ ] Add sparse matrix
   - [ ] Add sparse operations.
   - [ ] Add high level linear algebra operations.
   - [ ] Add benchmark
     
** Contact
   Red-Portal
   - msca8h@naver.com
   - msca8h@sogang.ac.kr
     
** License
   Copyright RedPortal 2017.

   Distributed under the Boost Software License, Version 1.0.
   (See accompanying file LICENSE or copy at
   http://www.boost.org/LICENSE_1_0.txt)

